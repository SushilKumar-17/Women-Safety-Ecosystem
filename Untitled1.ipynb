{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3d251e3-2637-426f-94a9-8248e3471b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\ybadr/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-9-6 Python-3.12.3 torch-2.4.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ybadr\\anaconda3\\Anaconda\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ybadr\\anaconda3\\Anaconda\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ybadr\\anaconda3\\Anaconda\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ybadr\\anaconda3\\Anaconda\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "All PyTorch model weights were used when initializing TFViTForImageClassification.\n",
      "\n",
      "All the weights of TFViTForImageClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFViTForImageClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "YOLO Detection Metrics:\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "\n",
      "Gender Classification Accuracy: 0.0000\n",
      "\n",
      "Fighting Detection Rate: 0.1113\n",
      "\n",
      "Modified Accuracy: 0.6111\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "import datetime\n",
    "import os\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "# Suppress the FutureWarning\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# YOLOv5 model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "model.classes = [0]  # Only detect persons (class 0 in COCO dataset)\n",
    "model.conf = 0.5  # Confidence threshold\n",
    "model.cpu()  # Ensure model runs on CPU\n",
    "\n",
    "# Load gender classification model\n",
    "gender_model = load_model('cctv_gender_classifier.h5')\n",
    "gender_classes = ['man', 'woman']\n",
    "\n",
    "# Initialize DeepSort\n",
    "tracker = DeepSort(max_age=10, \n",
    "                   n_init=3,\n",
    "                   nms_max_overlap=1.0,\n",
    "                   max_cosine_distance=0.3,\n",
    "                   nn_budget=None,\n",
    "                   override_track_class=None,\n",
    "                   embedder=\"mobilenet\",\n",
    "                   half=True,\n",
    "                   bgr=True,\n",
    "                   embedder_gpu=False,\n",
    "                   embedder_model_name=None,\n",
    "                   embedder_wts=None,\n",
    "                   polygon=False,\n",
    "                   today=None)\n",
    "\n",
    "# Load the action recognition pipeline\n",
    "action_pipe = pipeline(\"image-classification\", model=\"rvv-karma/Human-Action-Recognition-VIT-Base-patch16-224\", framework=\"tf\")\n",
    "\n",
    "def preprocess_body(body_crop, target_size=(126, 126)):\n",
    "    try:\n",
    "        body_crop = cv2.resize(body_crop, target_size)\n",
    "        body_crop = body_crop.astype(\"float32\") / 255.0\n",
    "        body_crop = img_to_array(body_crop)\n",
    "        body_crop = np.expand_dims(body_crop, axis=0)\n",
    "        return body_crop\n",
    "    except Exception as e:\n",
    "        print(f\"Error in preprocess_body: {e}\")\n",
    "        return None\n",
    "\n",
    "# Initialize variables\n",
    "total_persons = 0\n",
    "gender_counts = {'man': 0, 'woman': 0}\n",
    "frame_skip = 2\n",
    "processing_times = []\n",
    "fighting_count = 0\n",
    "lone_woman_flag = False\n",
    "surrounded_woman_flag = False\n",
    "sos_events = []\n",
    "alerts = []\n",
    "warnings = []\n",
    "gender_counts_over_time = []\n",
    "\n",
    "# New variables for performance metrics\n",
    "total_frames = 0\n",
    "correct_detections = 0\n",
    "false_positives = 0\n",
    "false_negatives = 0\n",
    "gender_correct = 0\n",
    "gender_total = 0\n",
    "\n",
    "# Open video file\n",
    "video = cv2.VideoCapture(r\"C:\\Users\\ybadr\\OneDrive\\Desktop\\SIH VIDS\\Horrifying CCTV footage shows girl abducted, molested in Bengaluru.mp4\")\n",
    "frame_count = 0\n",
    "\n",
    "# Rest of your existing code...\n",
    "\n",
    "while video.isOpened():\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    total_frames += 1\n",
    "    frame_count += 1\n",
    "    if frame_count % frame_skip != 0:\n",
    "        continue\n",
    "    \n",
    "    # Resize frame for faster processing\n",
    "    frame = cv2.resize(frame, (640, 480))\n",
    "    \n",
    "    # YOLOv5 detection\n",
    "    results = model(frame)\n",
    "    detections = results.xyxy[0].cpu().numpy()\n",
    "    \n",
    "    # Prepare detections for DeepSort\n",
    "    deepsort_detections = []\n",
    "    for det in detections:\n",
    "        x1, y1, x2, y2, conf, cls = det\n",
    "        deepsort_detections.append(([x1, y1, x2 - x1, y2 - y1], conf, int(cls)))\n",
    "    \n",
    "    # Update DeepSort\n",
    "    tracks = tracker.update_tracks(deepsort_detections, frame=frame)\n",
    "    \n",
    "    # Reset gender counts for each frame\n",
    "    gender_counts = {'man': 0, 'woman': 0}\n",
    "    \n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "        \n",
    "        track_id = track.track_id\n",
    "        ltrb = track.to_ltrb()\n",
    "        x1, y1, x2, y2 = map(int, ltrb)\n",
    "        \n",
    "        # Perform gender classification if not already done\n",
    "        if not hasattr(track, 'gender'):\n",
    "            body_crop = frame[y1:y2, x1:x2]\n",
    "            preprocessed_body = preprocess_body(body_crop)\n",
    "            if preprocessed_body is not None:\n",
    "                try:\n",
    "                    gender_conf = gender_model.predict(preprocessed_body)[0]\n",
    "                    gender_idx = np.argmax(gender_conf)\n",
    "                    \n",
    "                    if gender_conf[gender_idx] > 0.7:  # Confidence threshold\n",
    "                        gender_label = gender_classes[gender_idx]\n",
    "                        track.gender = gender_label\n",
    "                        track.gender_confidence = gender_conf[gender_idx]\n",
    "                        total_persons += 1\n",
    "                        \n",
    "                        # Update gender classification metrics\n",
    "                        gender_total += 1\n",
    "                        # Assume ground truth is available (you may need to modify this)\n",
    "                        if gender_label == \"actual_gender\":  # Replace with actual ground truth\n",
    "                            gender_correct += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in gender classification: {e}\")\n",
    "        \n",
    "        # Count genders in current frame\n",
    "        if hasattr(track, 'gender'):\n",
    "            gender_counts[track.gender] += 1\n",
    "        \n",
    "        # Draw bounding box and label\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        label = f\"ID {track_id}\"\n",
    "        if hasattr(track, 'gender'):\n",
    "            label += f\" ({track.gender})\"\n",
    "        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        # Update YOLO metrics (assuming ground truth is available)\n",
    "        correct_detections += 1  # This should be based on IoU with ground truth\n",
    "    \n",
    "    # Perform action recognition\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pil_image = Image.fromarray(rgb_frame)\n",
    "    action_result = action_pipe(pil_image)\n",
    "    \n",
    "    # Check for fighting action\n",
    "    sos_detected = False\n",
    "    for item in action_result:\n",
    "        if item['label'] == 'Fighting' and item['score'] > 0.97:\n",
    "            sos_detected = True\n",
    "            fighting_count += 1\n",
    "            # Rest of your SOS detection code...\n",
    "    \n",
    "    # Rest of your existing code...\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Calculate performance metrics\n",
    "yolo_precision = correct_detections / (correct_detections + false_positives) if (correct_detections + false_positives) > 0 else 0\n",
    "yolo_recall = correct_detections / (correct_detections + false_negatives) if (correct_detections + false_negatives) > 0 else 0\n",
    "yolo_f1 = 2 * (yolo_precision * yolo_recall) / (yolo_precision + yolo_recall) if (yolo_precision + yolo_recall) > 0 else 0\n",
    "\n",
    "gender_accuracy = gender_correct / gender_total if gender_total > 0 else 0\n",
    "\n",
    "fighting_accuracy = fighting_count / total_frames  # This is a simplified metric\n",
    "\n",
    "# Print performance metrics\n",
    "print(f\"YOLO Detection Metrics:\")\n",
    "print(f\"Precision: {yolo_precision:.4f}\")\n",
    "print(f\"Recall: {yolo_recall:.4f}\")\n",
    "print(f\"F1 Score: {yolo_f1:.4f}\")\n",
    "print(f\"\\nGender Classification Accuracy: {gender_accuracy:.4f}\")\n",
    "print(f\"\\nFighting Detection Rate: {fighting_accuracy:.4f}\")\n",
    "\n",
    "# Calculate modified accuracy (example: weighted average of YOLO and gender accuracy)\n",
    "modified_accuracy = 0.6 * yolo_f1 + 0.3 * gender_accuracy + 0.1 * fighting_accuracy\n",
    "print(f\"\\nModified Accuracy: {modified_accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "610a0564-8ecf-423c-b403-c90576c5604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the video file\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f061ccff-9792-49d0-983d-7936dfe05a68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
